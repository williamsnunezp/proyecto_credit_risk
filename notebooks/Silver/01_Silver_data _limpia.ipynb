{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39f762e-e4c0-4af2-8038-1b34beff1e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CAPA SILVER \n",
    "- Tabla de datos rechazados\n",
    "- Tabla de datos limpios identidicando Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2ff55f-cc49-4b64-8ae1-eba6df77c9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, trim, upper, when, lit, concat_ws, current_timestamp\n",
    "from pyspark.sql.functions import md5, concat_ws, col, dense_rank, monotonically_increasing_id\n",
    "from pyspark.sql.functions import percentile_approx, mean as spark_mean\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681aef7e-5f85-4de9-b897-e8de66e4e15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar Bronze\n",
    "df_bronze = spark.table(\"workspace.credit_risk.bronze_credit_risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a61bb5-a683-47f5-b2aa-2f61c32aaf55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_bronze\n",
    "    .select(\n",
    "        F.col(\"edad\").cast(\"int\"),\n",
    "        F.col(\"ingreso_anual\").cast(\"double\"),\n",
    "        F.col(\"tipo_vivienda\").alias(\"tipo_vivienda\"),\n",
    "        F.col(\"anios_empleo\").cast(\"double\"),\n",
    "        F.col(\"proposito\"),\n",
    "        F.col(\"calificacion\"),\n",
    "        F.col(\"monto\").cast(\"double\"),\n",
    "        F.col(\"tasa_interes\").cast(\"double\"),\n",
    "        F.col(\"estado_pago\").cast(\"int\"),\n",
    "        F.col(\"pct_ingreso\").cast(\"double\"),\n",
    "        F.col(\"historial_default\"),\n",
    "        F.col(\"anios_hist_credito\").cast(\"double\"),\n",
    "        F.col(\"fecha_ingesta\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b40dc4-203f-4bd8-a820-9ef1f46c7b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. IDENTIFICAR Y SEPARAR DUPLICADOS\n",
    "\n",
    "# Marcar duplicados (mantenemos el primero)\n",
    "window_spec = Window.partitionBy(\n",
    "    \"edad\", \"ingreso_anual\", \"tipo_vivienda\", \"anios_empleo\", \n",
    "    \"proposito\", \"calificacion\", \"monto\", \"tasa_interes\", \n",
    "    \"estado_pago\", \"pct_ingreso\", \"historial_default\", \"anios_hist_credito\"\n",
    ").orderBy(\"fecha_ingesta\")\n",
    "\n",
    "df_con_marca = df_silver.withColumn(\"fila_num\", row_number().over(window_spec))\n",
    "\n",
    "# Separar duplicados\n",
    "df_duplicados = df_con_marca.filter(col(\"fila_num\") > 1).withColumn(\"razon_rechazo\", lit(\"Duplicado\"))\n",
    "df_sin_duplicados = df_con_marca.filter(col(\"fila_num\") == 1).drop(\"fila_num\")\n",
    "\n",
    "print(f\"Duplicados identificados: {df_duplicados.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1946430d-4f24-44ea-9703-90597285eab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. IDENTIFICAR Y SEPARAR REGISTROS INCONSISTENTES\n",
    "\n",
    "df_inconsistentes = df_sin_duplicados.filter(\n",
    "    (col(\"edad\") < 18) | (col(\"edad\") > 100) |\n",
    "    (col(\"anios_empleo\") < 0) | (col(\"anios_empleo\") > 50)\n",
    ").withColumn(\"razon_rechazo\", \n",
    "    when((col(\"edad\") < 18) | (col(\"edad\") > 100), \"Edad fuera de rango\")\n",
    "    .when((col(\"anios_empleo\") < 0) | (col(\"anios_empleo\") > 50), \"Anios empleo inválidos\")\n",
    ")\n",
    "\n",
    "df_consistentes = df_sin_duplicados.filter(\n",
    "    (col(\"edad\") >= 18) & (col(\"edad\") <= 100) &\n",
    "    (col(\"anios_empleo\") >= 0) & (col(\"anios_empleo\") <= 50)\n",
    ")\n",
    "\n",
    "print(f\"Registros inconsistentes identificados: {df_inconsistentes.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a69457-3ddc-4991-8dfe-7ddfcf013aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tabla de rechazados (duplicados + inconsistentes)\n",
    "df_duplicados_clean = df_duplicados.drop(\"fila_num\")\n",
    "df_inconsistentes_clean = df_inconsistentes  # Ya no tiene fila_num\n",
    "df_rechazados = df_duplicados_clean.union(df_inconsistentes_clean)\\\n",
    "    .withColumn(\"fecha_rechazo\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60da58e7-f88f-4fc4-94c4-1dc1ccb86a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. LIMPIAR COLUMNAS STRING\n",
    "\n",
    "df_strings_limpios = (df_consistentes\n",
    "    .withColumn(\"tipo_vivienda\", upper(trim(col(\"tipo_vivienda\"))))\n",
    "    .withColumn(\"proposito\", upper(trim(col(\"proposito\"))))\n",
    "    .withColumn(\"calificacion\", upper(trim(col(\"calificacion\"))))\n",
    "    .withColumn(\"historial_default\", upper(trim(col(\"historial_default\"))))\n",
    ")\n",
    "\n",
    "# 4. IMPUTAR NULOS\n",
    "\n",
    "# Para años_empleo: imputar con mediana\n",
    "mediana_empleo = df_strings_limpios.approxQuantile(\"anios_empleo\", [0.5], 0.01)[0]\n",
    "\n",
    "# Para tasa_interes: imputar con mediana por calificación\n",
    "medianas_tasa = df_strings_limpios.groupBy(\"calificacion\").agg(\n",
    "    spark_mean(\"tasa_interes\").alias(\"tasa_media\")\n",
    ")\n",
    "\n",
    "df_con_imputacion = (df_strings_limpios\n",
    "    .withColumn(\"anios_empleo\", \n",
    "        when(col(\"anios_empleo\").isNull(), mediana_empleo).otherwise(col(\"anios_empleo\"))\n",
    "    )\n",
    "    .join(medianas_tasa, \"calificacion\", \"left\")\n",
    "    .withColumn(\"tasa_interes\",\n",
    "        when(col(\"tasa_interes\").isNull(), col(\"tasa_media\")).otherwise(col(\"tasa_interes\"))\n",
    "    )\n",
    "    .drop(\"tasa_media\")\n",
    ")\n",
    "\n",
    "print(f\"Nulos imputados - Años empleo: mediana={mediana_empleo}, Tasa interés: por calificación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a9cf0c-77ee-4177-970d-fc6973393380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4.5. GENERAR ID ÚNICO\n",
    "\n",
    "# ID ÚNICO DE REGISTRO de cliente: Cada fila es única\n",
    "df_con_ids = df_con_imputacion.withColumn(\n",
    "    \"id_cliente\",\n",
    "    monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "print(f\" ID generado:\")\n",
    "print(f\"   - id_cliente: basado en registros únicos\")\n",
    "\n",
    "# Verificar cuántos clientes únicos hay\n",
    "total_registros = df_con_ids.count()\n",
    "print(f\"\\n Total de registros: {total_registros:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b5ec0c-ed9f-4f0c-aa08-e77000c3986e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. CALCULAR LÍMITES PARA OUTLIERS (IQR)\n",
    "\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "columnas_outliers = [\"edad\", \"ingreso_anual\", \"anios_empleo\", \"monto\", \"pct_ingreso\", \"anios_hist_credito\"]\n",
    "\n",
    "# Calcular Q1, Q3 para cada columna\n",
    "limites = {}\n",
    "for col_name in columnas_outliers:\n",
    "    stats = df_con_ids.select(\n",
    "        percentile_approx(col_name, 0.25).alias(\"Q1\"),\n",
    "        percentile_approx(col_name, 0.75).alias(\"Q3\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    Q1 = stats[\"Q1\"]\n",
    "    Q3 = stats[\"Q3\"]\n",
    "    IQR = Q3 - Q1\n",
    "    limites[col_name] = {\n",
    "        \"lower\": Q1 - 1.7 * IQR,\n",
    "        \"upper\": Q3 + 1.7 * IQR\n",
    "    }\n",
    "    print(f\"{col_name}: Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a653a41f-5215-469e-8c6e-6a1dbe5052f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. IDENTIFICAR OUTLIERS CON FLAG\n",
    "\n",
    "# Crear condición para marcar outliers\n",
    "condicion_outlier = None\n",
    "for col_name, bounds in limites.items():\n",
    "    condicion = (col(col_name) < bounds[\"lower\"]) | (col(col_name) > bounds[\"upper\"])\n",
    "    condicion_outlier = condicion if condicion_outlier is None else (condicion_outlier | condicion)\n",
    "\n",
    "# Agregar columna flag de outlier\n",
    "df_con_flag = df_con_ids.withColumn(\n",
    "    \"es_outlier\", \n",
    "    when(condicion_outlier, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "count_outliers = df_con_flag.filter(col(\"es_outlier\") == 1).count()\n",
    "count_normales = df_con_flag.filter(col(\"es_outlier\") == 0).count()\n",
    "\n",
    "print(f\"\\n Outliers identificados: {count_outliers:,} ({round(count_outliers/df_con_flag.count()*100, 2)}%)\")\n",
    "print(f\" Registros normales: {count_normales:,} ({round(count_normales/df_con_flag.count()*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed7334-0293-46b0-9561-0797d89af6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 4. GUARDAR EN TABLAS SILVER\n",
    "\n",
    "    # Tabla de rechazados (duplicados + inconsistentes)\n",
    "    df_rechazados.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"workspace.credit_risk.silver_registros_rechazados\")\n",
    "\n",
    "\n",
    "    # Tabla principal: datos limpios con flag de outliers\n",
    "    df_con_flag.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"workspace.credit_risk.silver_credit_risk_limpio\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    # Tipo de error\n",
    "    error_type = type(e).__name__\n",
    "    # Descripcion de error\n",
    "    error_summary = str(e)\n",
    "    # Trazar el error\n",
    "    error_trace = traceback.format_exc()\n",
    "\n",
    "    # Error completo\n",
    "    error_msg_full = \"f{error_type}: {error_sumamary}/n{error_trace}\"\n",
    "\n",
    "    if len(error_msg_full) > 500:\n",
    "        error_msg = error_msg_full[:500]+\"\\n[...]Error Truncado[...]\"\n",
    "    else:\n",
    "        error_msg = error_msg_full\n",
    "\n",
    "    dbutils.jobs.taskValues.set(key=\"error\", value=error_msg)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Silver_data _limpia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
